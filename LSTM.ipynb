{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260c0939",
   "metadata": {},
   "source": [
    "# Updated LSTM Model Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e9b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8dd5e",
   "metadata": {},
   "source": [
    "## Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60     # Sequence length matching the data preparation\n",
    "prediction_horizon = 25  # Matching Galformer prediction horizon\n",
    "batch_size = 512         # Matching Galformer batch size\n",
    "epochs = 15              # Number of training epochs matching Galformer\n",
    "print('Batch size:', batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0bc12",
   "metadata": {},
   "source": [
    "## Define Feature Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91610204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature description for parsing TFRecord files\n",
    "feature_description = {\n",
    "    'feature': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ba6df",
   "metadata": {},
   "source": [
    "## Get the List of TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all TFRecord files\n",
    "tfrecord_files = glob.glob('tfrecords_data/*.tfrecord')\n",
    "print(f\"Found {len(tfrecord_files)} TFRecord files.\")\n",
    "\n",
    "# Create a dataset from the list of TFRecord files\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_files, compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381e935",
   "metadata": {},
   "source": [
    "## Determine `num_features` from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a593ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Extract one example to determine num_features\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.io.parse_single_example(raw_record, feature_description)\n",
    "    feature = tf.io.parse_tensor(example['feature'], out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n",
    "    sequence_length = feature.shape[0]\n",
    "    num_features = feature.shape[1]\n",
    "    prediction_horizon = label.shape[0]\n",
    "    print(f\"Sequence Length: {sequence_length}, Num Features: {num_features}, Prediction Horizon: {prediction_horizon}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05356751",
   "metadata": {},
   "source": [
    "## Function to Parse and Normalize TFRecord Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136795c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example_proto):\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    feature = tf.io.parse_tensor(example['feature'], out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n",
    "\n",
    "    # Set shapes for feature and label\n",
    "    feature.set_shape([sequence_length, num_features])\n",
    "    label.set_shape([prediction_horizon])\n",
    "\n",
    "    # Normalize features (z-score standardization)\n",
    "    feature_mean = tf.reduce_mean(feature, axis=0, keepdims=True)\n",
    "    feature_std = tf.math.reduce_std(feature, axis=0, keepdims=True) + 1e-6  # Add epsilon to avoid division by zero\n",
    "    feature = (feature - feature_mean) / feature_std\n",
    "\n",
    "    # Normalize labels (z-score standardization)\n",
    "    label_mean = tf.reduce_mean(label)\n",
    "    label_std = tf.math.reduce_std(label) + 1e-6\n",
    "    label = (label - label_mean) / label_std\n",
    "\n",
    "    # Store label mean and std for inverse transformation (if needed)\n",
    "    label_info = tf.stack([label_mean, label_std])  # Shape: (2,)\n",
    "\n",
    "    return feature, (label, label_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11828a3",
   "metadata": {},
   "source": [
    "## Create `tf.data.Dataset` from TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be539f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the serialized data in the TFRecord files\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Determine the total dataset size\n",
    "total_dataset_size = sum(1 for _ in parsed_dataset)\n",
    "print(f\"Total number of samples in dataset: {total_dataset_size}\")\n",
    "\n",
    "# Reset the parsed_dataset iterator after counting\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04d269",
   "metadata": {},
   "source": [
    "## Split Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c59e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Shuffle and split the dataset\n",
    "train_size = int(0.8 * total_dataset_size)\n",
    "test_size = total_dataset_size - train_size\n",
    "\n",
    "# Shuffle the entire dataset and split\n",
    "parsed_dataset = parsed_dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=False)\n",
    "\n",
    "train_dataset = parsed_dataset.take(train_size)\n",
    "test_dataset = parsed_dataset.skip(train_size)\n",
    "\n",
    "# Batch and prefetch the datasets\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Training samples: {train_size}, Testing samples: {test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f9840",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfc663",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to extract labels (normalized labels are in y[0])\n",
    "def strip_label_info(x, y):\n",
    "    return x, y[0]\n",
    "\n",
    "train_dataset_for_training = train_dataset.map(strip_label_info)\n",
    "test_dataset_for_training = test_dataset.map(strip_label_info)\n",
    "\n",
    "# Get input_shape and output_length from the dataset\n",
    "for features, labels in train_dataset_for_training.take(1):\n",
    "    print(f\"Features shape: {features.shape}, Labels shape: {labels.shape}\")\n",
    "    input_shape = features.shape[1:]  # Exclude batch dimension\n",
    "    output_length = labels.shape[1]\n",
    "    num_features = input_shape[1]\n",
    "    print(f\"Input shape: {input_shape}, Output length: {output_length}, Number of features: {num_features}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89a240",
   "metadata": {},
   "source": [
    "## Build and Train the Enhanced LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the enhanced LSTM model\n",
    "def build_enhanced_lstm_model(input_shape, output_length):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), input_shape=input_shape),\n",
    "        Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Bidirectional(LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_length, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Define learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,  # Matching Galformer learning rate\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "model = build_enhanced_lstm_model(input_shape, output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63052d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping and Learning Rate Scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [early_stopping, lr_scheduler]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset_for_training,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_dataset_for_training,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6ed74",
   "metadata": {},
   "source": [
    "## Plot Training & Validation Loss Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d48abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Enhanced LSTM Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('enhanced_lstm_train_validation_loss.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac6797",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1840ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = model.evaluate(test_dataset_for_training, verbose=1)\n",
    "print(f\"Enhanced LSTM Model - Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750556db",
   "metadata": {},
   "source": [
    "## Predict on Test Data and Denormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_list = []\n",
    "y_test_list = []\n",
    "label_info_list = []\n",
    "\n",
    "for features, (labels, label_info) in test_dataset:\n",
    "    X_test_list.append(features.numpy())\n",
    "    y_test_list.append(labels.numpy())\n",
    "    label_info_list.append(label_info.numpy())\n",
    "\n",
    "# Concatenate lists to form arrays\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "label_info = np.concatenate(label_info_list, axis=0)  # Shape: (num_samples, 2)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Denormalize predictions and actual labels\n",
    "label_mean = label_info[:, 0]  # Shape: (num_samples,)\n",
    "label_std = label_info[:, 1]   # Shape: (num_samples,)\n",
    "\n",
    "# Reshape label_mean and label_std to (num_samples, 1) to match the predictions\n",
    "label_mean = label_mean.reshape(-1, 1)\n",
    "label_std = label_std.reshape(-1, 1)\n",
    "\n",
    "# Denormalize predictions and actual labels\n",
    "y_pred_denorm = y_pred * label_std + label_mean\n",
    "y_test_denorm = y_test * label_std + label_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16cea7",
   "metadata": {},
   "source": [
    "## Visualize Predictions for First Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Actual Prices for the first test sample\n",
    "plt.plot(range(1, output_length + 1), y_test_denorm[0], label=\"Actual Prices\", marker='o')\n",
    "\n",
    "# Plot Predicted Prices for the first test sample\n",
    "plt.plot(range(1, output_length + 1), y_pred_denorm[0], label=\"Predicted Prices (Enhanced LSTM)\", marker='x')\n",
    "\n",
    "plt.title(\"Actual vs Predicted Prices (First Test Sample - Enhanced LSTM)\")\n",
    "plt.xlabel(\"Days Ahead\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.savefig('enhanced_lstm_actual_vs_predicted_prices.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508b493",
   "metadata": {},
   "source": [
    "## Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe5b53",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "model.save('enhanced_stock_lstm_model.keras')\n",
    "print(\"Enhanced LSTM Model has been saved to 'enhanced_stock_lstm_model.keras'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec526e2",
   "metadata": {},
   "source": [
    "## Inference with New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b473536",
   "metadata": {},
   "source": [
    "### Prepare Inference Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fae44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_inference_data(data, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Prepare input data for inference for a single company.\n",
    "    Args:\n",
    "        data (DataFrame): The DataFrame containing historical stock data.\n",
    "        sequence_length (int): The number of past days to consider as input.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: The input data ready for prediction.\n",
    "    \"\"\"\n",
    "    required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "    # Check if required columns are present\n",
    "    if not all(col in data.columns for col in required_columns):\n",
    "        print(f\"Some required columns are missing. Needed: {required_columns}\")\n",
    "        return None\n",
    "\n",
    "    if len(data) < sequence_length:\n",
    "        print(f\"Not enough data to create input sequence.\")\n",
    "        return None\n",
    "\n",
    "    # Prepare input features\n",
    "    input_features = data[required_columns].values.astype(np.float32)\n",
    "\n",
    "    # Take the last `sequence_length` days as input\n",
    "    input_sequence = input_features[-sequence_length:]\n",
    "\n",
    "    # Normalize features (z-score standardization)\n",
    "    mean = np.mean(input_sequence, axis=0, keepdims=True)\n",
    "    std = np.std(input_sequence, axis=0, keepdims=True) + 1e-6  # To avoid division by zero\n",
    "    normalized_sequence = (input_sequence - mean) / std\n",
    "\n",
    "    # Reshape to match model input\n",
    "    input_sequence = normalized_sequence.reshape(1, sequence_length, -1)\n",
    "    return input_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4fe0d",
   "metadata": {},
   "source": [
    "### Get Enhanced LSTM Predictions for a Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd8289",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_enhanced_lstm_predictions_for_company(data, lstm_model, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Get Enhanced LSTM predictions for a single company using the trained model.\n",
    "    Args:\n",
    "        data (DataFrame): The DataFrame containing historical stock data.\n",
    "        lstm_model: Trained Enhanced LSTM model.\n",
    "        sequence_length (int): Number of past days to consider as input.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Predicted prices.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare data for inference\n",
    "        input_data = prepare_inference_data(data, sequence_length=sequence_length)\n",
    "        if input_data is None:\n",
    "            return None\n",
    "\n",
    "        # Make predictions with Enhanced LSTM\n",
    "        pred_lstm = lstm_model.predict(input_data)\n",
    "\n",
    "        # Denormalize predictions\n",
    "        recent_close_prices = data['Close'].values[-prediction_horizon:]\n",
    "        label_mean = np.mean(recent_close_prices)\n",
    "        label_std = np.std(recent_close_prices) + 1e-6\n",
    "        pred_lstm_denorm = pred_lstm * label_std + label_mean\n",
    "\n",
    "        return pred_lstm_denorm.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error making LSTM predictions for the company: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a7697",
   "metadata": {},
   "source": [
    "### Load the Trained Enhanced LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f80f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.models.load_model('enhanced_stock_lstm_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e8190",
   "metadata": {},
   "source": [
    "### Making Predictions for Multiple Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c3d39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'GOOGL']  # List of tickers\n",
    "all_predictions = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - timedelta(days=730)  # Increased data period for better prediction\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    if data.empty:\n",
    "        print(f\"No data available for {ticker}.\")\n",
    "        continue\n",
    "    predictions = get_enhanced_lstm_predictions_for_company(data, lstm_model, sequence_length=60)\n",
    "    if predictions is not None:\n",
    "        all_predictions[ticker] = predictions\n",
    "    else:\n",
    "        print(f\"Predictions not available for {ticker}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9c5a0",
   "metadata": {},
   "source": [
    "### Saving Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to DataFrame for further analysis or saving\n",
    "def predictions_to_dataframe(predictions_dict):\n",
    "    records = []\n",
    "    for ticker, pred_values in predictions_dict.items():\n",
    "        for day_ahead, value in enumerate(pred_values, start=1):\n",
    "            records.append({\n",
    "                'Ticker': ticker,\n",
    "                'Day_Ahead': day_ahead,\n",
    "                'Predicted_Price': value\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "predictions_df = predictions_to_dataframe(all_predictions)\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fdce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions DataFrame to a CSV file\n",
    "predictions_df.to_csv('enhanced_lstm_stock_price_predictions.csv', index=False)\n",
    "print(\"Enhanced LSTM Predictions have been saved to 'enhanced_lstm_stock_price_predictions.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
