{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6312f5",
   "metadata": {},
   "source": [
    "# Updated Galformer Model Training on CPU\n",
    "\n",
    "This notebook trains an improved Transformer-based model (Galformer) for stock price prediction.\n",
    "The improvements include data normalization, model architecture enhancements, and training optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bc2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if necessary\n",
    "! pip install tensorflow==2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d17bf",
   "metadata": {},
   "source": [
    "## Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60  # Should match the value used in data preparation\n",
    "prediction_horizon = 5\n",
    "epochs = 10\n",
    "batch_size = 512  # Increased batch size\n",
    "print('Batch size:', batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338e062",
   "metadata": {},
   "source": [
    "## Define Feature Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63396cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature description for parsing TFRecord files\n",
    "feature_description = {\n",
    "    'feature': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5b8f6",
   "metadata": {},
   "source": [
    "## Get the List of TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1171455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all TFRecord files\n",
    "tfrecord_files = glob.glob('tfrecords_data/*.tfrecord')\n",
    "print(f\"Found {len(tfrecord_files)} TFRecord files.\")\n",
    "\n",
    "# Create a dataset from the list of TFRecord files\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_files, compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb130cce",
   "metadata": {},
   "source": [
    "## Determine `num_features` from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03057f5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Extract one example to determine num_features\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.io.parse_single_example(raw_record, feature_description)\n",
    "    feature = tf.io.parse_tensor(example['feature'], out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n",
    "    sequence_length = feature.shape[0]\n",
    "    num_features = feature.shape[1]\n",
    "    prediction_horizon = label.shape[0]\n",
    "    print(f\"Sequence Length: {sequence_length}, Num Features: {num_features}, Prediction Horizon: {prediction_horizon}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a3a9a",
   "metadata": {},
   "source": [
    "## Function to Parse and Normalize TFRecord Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa7a51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example_proto):\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    feature = tf.io.parse_tensor(example['feature'], out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n",
    "\n",
    "    # Set shapes for feature and label\n",
    "    feature.set_shape([sequence_length, num_features])\n",
    "    label.set_shape([prediction_horizon])\n",
    "\n",
    "    # Normalize features (z-score standardization)\n",
    "    feature_mean = tf.reduce_mean(feature, axis=0, keepdims=True)\n",
    "    feature_std = tf.math.reduce_std(feature, axis=0, keepdims=True) + 1e-6  # Add epsilon to avoid division by zero\n",
    "    feature = (feature - feature_mean) / feature_std\n",
    "\n",
    "    # Normalize labels (z-score standardization)\n",
    "    label_mean = tf.reduce_mean(label, keepdims=True)\n",
    "    label_std = tf.math.reduce_std(label, keepdims=True) + 1e-6\n",
    "    label = (label - label_mean) / label_std\n",
    "\n",
    "    # Store label mean and std for inverse transformation (if needed)\n",
    "    label_info = tf.stack([label_mean, label_std], axis=0)\n",
    "\n",
    "    return feature, (label, label_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f1b71",
   "metadata": {},
   "source": [
    "## Create `tf.data.Dataset` from TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the serialized data in the TFRecord files\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Determine the total dataset size\n",
    "total_dataset_size = sum(1 for _ in parsed_dataset)\n",
    "print(f\"Total number of samples in dataset: {total_dataset_size}\")\n",
    "\n",
    "# Reset the parsed_dataset iterator after counting\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fc2d5",
   "metadata": {},
   "source": [
    "## Split Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027e721",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Shuffle and split the dataset\n",
    "train_size = int(0.8 * total_dataset_size)\n",
    "test_size = total_dataset_size - train_size\n",
    "\n",
    "# Shuffle the entire dataset and split\n",
    "parsed_dataset = parsed_dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=False)\n",
    "\n",
    "train_dataset = parsed_dataset.take(train_size)\n",
    "test_dataset = parsed_dataset.skip(train_size)\n",
    "\n",
    "# Batch and prefetch the datasets\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Training samples: {train_size}, Testing samples: {test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e6ee4",
   "metadata": {},
   "source": [
    "## Add Positional Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a27249",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def positional_encoding(sequence_length, d_model):\n",
    "    position = tf.range(sequence_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "    angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    angle_rads = position * angle_rates\n",
    "    sin_terms = tf.sin(angle_rads[:, 0::2])\n",
    "    cos_terms = tf.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = tf.concat([sin_terms, cos_terms], axis=-1)\n",
    "    return pos_encoding  # Shape: (sequence_length, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d55a40",
   "metadata": {},
   "source": [
    "## Build and Compile the Enhanced Galformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Galformer model\n",
    "input_shape = (sequence_length, num_features)  # (sequence_length, num_features)\n",
    "output_length = prediction_horizon  # prediction_horizon\n",
    "\n",
    "def build_galformer_model(input_shape, output_length, num_layers=3, dff=512, num_heads=8):\n",
    "    inputs = Input(shape=input_shape)  # input_shape = (sequence_length, num_features)\n",
    "\n",
    "    # Create positional encodings\n",
    "    pe = positional_encoding(input_shape[0], input_shape[1])\n",
    "    pe = tf.expand_dims(pe, axis=0)  # Shape: (1, sequence_length, num_features)\n",
    "\n",
    "    # Add positional encoding to inputs\n",
    "    x = inputs + pe  # Broadcasting, pe shape is (1, sequence_length, num_features)\n",
    "\n",
    "    # Stack multiple Transformer Encoder Layers\n",
    "    for _ in range(num_layers):\n",
    "        # Multi-Head Attention\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[1], dropout=0.1)(x, x)\n",
    "        attn_output = Dropout(0.2)(attn_output)  # Increased dropout\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
    "\n",
    "        # Feed Forward Network with L2 Regularization\n",
    "        ffn_output = Dense(dff, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(out1)\n",
    "        ffn_output = Dense(input_shape[1], kernel_regularizer=regularizers.l2(1e-4))(ffn_output)\n",
    "        ffn_output = Dropout(0.2)(ffn_output)  # Increased dropout\n",
    "        x = LayerNormalization(epsilon=1e-6)(ffn_output + out1)\n",
    "\n",
    "    # Flatten and Output Layer\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(output_length, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # Define learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,  # Reduced learning rate\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "galformer_model = build_galformer_model(input_shape, output_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386110b",
   "metadata": {},
   "source": [
    "## Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b67f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increased early stopping patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd4888",
   "metadata": {},
   "source": [
    "## Train the Enhanced Galformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0af006",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_galformer = galformer_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,  # Increased epochs\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1ef87",
   "metadata": {},
   "source": [
    "## Plot Training & Validation Loss Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_galformer.history['loss'], label='Train Loss')\n",
    "plt.plot(history_galformer.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Enhanced Galformer Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfae583",
   "metadata": {},
   "source": [
    "## Evaluate the Enhanced Galformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37180d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_galformer, test_mae_galformer = galformer_model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"Enhanced Galformer Model - Test Loss: {test_loss_galformer:.4f}, Test MAE: {test_mae_galformer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654ae7c",
   "metadata": {},
   "source": [
    "## Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70865bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect features, labels, and label info from test_dataset\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "label_info_list = []\n",
    "\n",
    "for features, (labels, label_info) in test_dataset:\n",
    "    X_test_list.append(features.numpy())\n",
    "    y_test_list.append(labels.numpy())\n",
    "    label_info_list.append(label_info.numpy())\n",
    "\n",
    "# Concatenate lists to form arrays\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "label_info = np.concatenate(label_info_list, axis=0)  # Shape: (num_samples, 2)\n",
    "\n",
    "# Predict on test data\n",
    "def add_positional_encoding(inputs):\n",
    "    pe = positional_encoding(inputs.shape[1], inputs.shape[2])\n",
    "    pe = tf.expand_dims(pe, axis=0)  # Shape: (1, sequence_length, num_features)\n",
    "    inputs_with_pe = inputs + pe.numpy()\n",
    "    return inputs_with_pe\n",
    "\n",
    "X_test_with_pe = add_positional_encoding(X_test)\n",
    "y_pred_galformer = galformer_model.predict(X_test_with_pe)\n",
    "\n",
    "# Denormalize predictions and actual labels\n",
    "y_pred_galformer_denorm = y_pred_galformer * label_info[:, 1:2] + label_info[:, 0:1]\n",
    "y_test_denorm = y_test * label_info[:, 1:2] + label_info[:, 0:1]\n",
    "\n",
    "# Visualize predictions for the first test sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Actual Prices for the first test sample\n",
    "plt.plot(range(1, output_length + 1), y_test_denorm[0], label=\"Actual Prices\", marker='o')\n",
    "\n",
    "# Plot Predicted Prices for the first test sample (Galformer)\n",
    "plt.plot(range(1, output_length + 1), y_pred_galformer_denorm[0], label=\"Predicted Prices (Galformer)\", marker='x')\n",
    "\n",
    "plt.title(\"Actual vs Predicted Prices (First Test Sample - Enhanced Galformer)\")\n",
    "plt.xlabel(\"Days Ahead\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb7882",
   "metadata": {},
   "source": [
    "## Save the Enhanced Galformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c2c3e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "galformer_model.save('enhanced_stock_galformer_model.keras')\n",
    "print(\"Enhanced Galformer Model has been saved to 'enhanced_stock_galformer_model.keras'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e42ee7",
   "metadata": {},
   "source": [
    "## Inference with New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390976f",
   "metadata": {},
   "source": [
    "### Load Company Data for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c31041",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_company_data():\n",
    "    all_dfs = {}\n",
    "    parquet_files = glob.glob('df_*.parquet')\n",
    "    for file in parquet_files:\n",
    "        key = file.split('.')[0]  # e.g., 'df_AAPL'\n",
    "        df = pd.read_parquet(file)\n",
    "        all_dfs[key] = df\n",
    "    return all_dfs\n",
    "\n",
    "all_dfs = load_company_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3992a",
   "metadata": {},
   "source": [
    "### Prepare Data for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bcc48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_inference_data(company_df, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Prepare input data for inference for a single company.\n",
    "    Args:\n",
    "        company_df (DataFrame): The DataFrame for a specific company.\n",
    "        sequence_length (int): The number of past days to consider as input.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: The input data ready for prediction.\n",
    "    \"\"\"\n",
    "    # Ensure data is sorted by date\n",
    "    company_df = company_df.sort_index()\n",
    "\n",
    "    # Select relevant input features (exclude targets)\n",
    "    input_features = company_df.filter(regex=\"^(?!.*target).*\").values\n",
    "\n",
    "    # Take the last `sequence_length` days as input for prediction\n",
    "    if len(input_features) >= sequence_length:\n",
    "        input_sequence = input_features[-sequence_length:]\n",
    "        # Normalize features using the same method as during training\n",
    "        feature_mean = np.mean(input_sequence, axis=0, keepdims=True)\n",
    "        feature_std = np.std(input_sequence, axis=0, keepdims=True) + 1e-6\n",
    "        input_sequence = (input_sequence - feature_mean) / feature_std\n",
    "        # Ensure the input sequence has the correct shape\n",
    "        input_sequence = np.expand_dims(input_sequence, axis=0)  # Add batch dimension\n",
    "        return input_sequence.astype(np.float32), feature_mean, feature_std  # Return mean and std for future use\n",
    "    else:\n",
    "        raise ValueError(\"Insufficient data for inference (less than sequence length).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71501c",
   "metadata": {},
   "source": [
    "### Make Predictions for Each Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80219f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_galformer_predictions_for_company(company_df, galformer_model, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Get Galformer predictions for a single company using the trained model.\n",
    "    Args:\n",
    "        company_df (DataFrame): DataFrame of the company.\n",
    "        galformer_model: Trained Galformer model.\n",
    "        sequence_length (int): Number of past days to consider as input.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Predicted prices.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare data for inference\n",
    "        input_data, _, _ = prepare_inference_data(company_df, sequence_length=sequence_length)\n",
    "\n",
    "        # Add positional encoding to inference data\n",
    "        pe = positional_encoding(input_data.shape[1], input_data.shape[2])\n",
    "        pe = tf.expand_dims(pe, axis=0)  # Shape: (1, sequence_length, num_features)\n",
    "        input_data_with_pe = input_data + pe.numpy()\n",
    "\n",
    "        # Make predictions with Galformer\n",
    "        pred_galformer = galformer_model.predict(input_data_with_pe)\n",
    "        # Since labels were normalized during training, you might need to apply inverse transformation here\n",
    "        # However, during inference, we don't have label_mean and label_std, so predictions will be in standardized form\n",
    "        # You may need to collect actual label_mean and label_std from training data and apply inverse transformation\n",
    "        # For now, we will return the standardized predictions\n",
    "        return pred_galformer.flatten()\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbdd08",
   "metadata": {},
   "source": [
    "### Load the Trained Enhanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fe995",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the trained Enhanced Galformer model\n",
    "galformer_model = tf.keras.models.load_model('enhanced_stock_galformer_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536291a",
   "metadata": {},
   "source": [
    "### Make Predictions for All Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2918c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_galformer_predictions_for_all_companies(all_dfs, galformer_model, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Get Galformer predictions for all companies.\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "    for company_key, company_df in all_dfs.items():\n",
    "        print(f\"Processing {company_key}...\")\n",
    "        predictions = get_galformer_predictions_for_company(company_df, galformer_model, sequence_length=sequence_length)\n",
    "        if predictions is not None:\n",
    "            all_predictions[company_key] = predictions\n",
    "        else:\n",
    "            print(f\"Predictions not available for {company_key}.\")\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319787f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Get predictions for all companies\n",
    "all_company_predictions = get_galformer_predictions_for_all_companies(all_dfs, galformer_model, sequence_length=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5656778",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_dataframe(predictions_dict):\n",
    "    \"\"\"\n",
    "    Convert predictions dictionary to a DataFrame.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for company_key, pred_values in predictions_dict.items():\n",
    "        for day_ahead, value in enumerate(pred_values, start=1):\n",
    "            records.append({\n",
    "                'Company': company_key.replace('df_', ''),\n",
    "                'Day_Ahead': day_ahead,\n",
    "                'Predicted_Price_Standardized': value  # Note that this is in standardized form\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "predictions_df = predictions_to_dataframe(all_company_predictions)\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c50998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions DataFrame to a CSV file\n",
    "predictions_df.to_csv('enhanced_galformer_stock_price_predictions.csv', index=False)\n",
    "print(\"Enhanced Galformer Predictions have been saved to 'enhanced_galformer_stock_price_predictions.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
