{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, MultiHeadAttention, LayerNormalization, Add\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124428f",
   "metadata": {},
   "source": [
    "## Constants and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ccc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60  # Should match the value used in Part 1\n",
    "prediction_horizon = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe444ba3",
   "metadata": {},
   "source": [
    "## Define Feature Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature description\n",
    "feature_description = {\n",
    "    'feature': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d5b3e",
   "metadata": {},
   "source": [
    "## Get the List of TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_files = glob.glob('tfrecords_data/*.tfrecord')\n",
    "\n",
    "# Create a dataset from the list of TFRecord files\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_files, compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3b861",
   "metadata": {},
   "source": [
    "## Determine `num_features` from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678134d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Extract one example to determine num_features\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    example = tf.io.parse_single_example(raw_record, feature_description)\n",
    "    feature = tf.io.parse_tensor(example['feature'], out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n",
    "    sequence_length = feature.shape[0]\n",
    "    num_features = feature.shape[1]\n",
    "    prediction_horizon = label.shape[0]\n",
    "    print(f\"Sequence Length: {sequence_length}, Num Features: {num_features}, Prediction Horizon: {prediction_horizon}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e52ee4",
   "metadata": {},
   "source": [
    "## Function to Parse TFRecord Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af36fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example_proto):\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    feature = tf.io.parse_tensor(example['feature'], out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(example['label'], out_type=tf.float32)\n",
    "    \n",
    "    # Set shapes for feature and label\n",
    "    feature.set_shape([sequence_length, num_features])\n",
    "    label.set_shape([prediction_horizon])\n",
    "    \n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c3efd",
   "metadata": {},
   "source": [
    "## Create tf.data.Dataset from TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96414595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the serialized data in the TFRecord files\n",
    "parsed_dataset = raw_dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Determine the total dataset size\n",
    "total_dataset_size = sum(1 for _ in parsed_dataset)\n",
    "print(f\"Total number of samples in dataset: {total_dataset_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad6ea5",
   "metadata": {},
   "source": [
    "## Split Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce92df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Shuffle and split the dataset\n",
    "train_size = int(0.8 * total_dataset_size)\n",
    "test_size = total_dataset_size - train_size\n",
    "\n",
    "# Shuffle the entire dataset and split\n",
    "parsed_dataset = parsed_dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=False)\n",
    "\n",
    "train_dataset = parsed_dataset.take(train_size)\n",
    "test_dataset = parsed_dataset.skip(train_size)\n",
    "\n",
    "# Batch and prefetch the datasets\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Training samples: {train_size}, Testing samples: {test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eda37c",
   "metadata": {},
   "source": [
    "## Add Positional Encoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ccd53",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def positional_encoding(sequence_length, d_model):\n",
    "    position = tf.range(sequence_length, dtype=tf.float32)[:, tf.newaxis]\n",
    "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "    angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    angle_rads = position * angle_rates\n",
    "    sin_terms = tf.sin(angle_rads[:, 0::2])\n",
    "    cos_terms = tf.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = tf.concat([sin_terms, cos_terms], axis=-1)\n",
    "    return pos_encoding  # Shape: (sequence_length, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce8a42",
   "metadata": {},
   "source": [
    "## Build and Train the Galformer Model (Transformer-based Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available GPUs\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2464136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory growth for GPUs\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Galformer model\n",
    "input_shape = (sequence_length, num_features)  # (sequence_length, num_features)\n",
    "output_length = prediction_horizon  # prediction_horizon\n",
    "\n",
    "def build_galformer_model(input_shape, output_length):\n",
    "    inputs = Input(shape=input_shape)  # input_shape = (sequence_length, num_features)\n",
    "    \n",
    "    # Create positional encodings\n",
    "    pe = positional_encoding(input_shape[0], input_shape[1])\n",
    "    pe = tf.expand_dims(pe, axis=0)  # Shape: (1, sequence_length, num_features)\n",
    "    \n",
    "    # Add positional encoding to inputs\n",
    "    x = inputs + pe  # Broadcasting, pe shape is (1, sequence_length, num_features)\n",
    "    \n",
    "    # Transformer Encoder Layer\n",
    "    attn_output = MultiHeadAttention(num_heads=4, key_dim=input_shape[1])(x, x)\n",
    "    attn_output = Dropout(0.1)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(attn_output + x)\n",
    "\n",
    "    # Feed Forward Network\n",
    "    ffn_output = Dense(128, activation='relu')(out1)\n",
    "    ffn_output = Dense(input_shape[1])(ffn_output)\n",
    "    ffn_output = Dropout(0.1)(ffn_output)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(ffn_output + out1)\n",
    "\n",
    "    # Flatten and Output Layer\n",
    "    x = Flatten()(out2)\n",
    "    outputs = Dense(output_length, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "galformer_model = build_galformer_model(input_shape, output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Galformer model\n",
    "history_galformer = galformer_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83778a02",
   "metadata": {},
   "source": [
    "## Plot Training & Validation Loss Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_galformer.history['loss'], label='Train Loss')\n",
    "plt.plot(history_galformer.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Galformer Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697044c",
   "metadata": {},
   "source": [
    "## Evaluate the Galformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_galformer, test_mae_galformer = galformer_model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"Galformer Model - Test Loss: {test_loss_galformer:.4f}, Test MAE: {test_mae_galformer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93efb36",
   "metadata": {},
   "source": [
    "## Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ebe533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect features and labels from test_dataset\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for features, labels in test_dataset:\n",
    "    X_test_list.append(features.numpy())\n",
    "    y_test_list.append(labels.numpy())\n",
    "\n",
    "# Concatenate lists to form arrays\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "# Predict on test data\n",
    "def add_positional_encoding(inputs):\n",
    "    pe = positional_encoding(inputs.shape[1], inputs.shape[2])\n",
    "    pe = tf.expand_dims(pe, axis=0)  # Shape: (1, sequence_length, num_features)\n",
    "    inputs_with_pe = inputs + pe.numpy()\n",
    "    return inputs_with_pe\n",
    "\n",
    "X_test_with_pe = add_positional_encoding(X_test)\n",
    "y_pred_galformer = galformer_model.predict(X_test_with_pe)\n",
    "\n",
    "# Visualize predictions for the first test sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Actual Prices for the first test sample\n",
    "plt.plot(range(1, output_length + 1), y_test[0], label=\"Actual Prices\", marker='o')\n",
    "\n",
    "# Plot Predicted Prices for the first test sample (Galformer)\n",
    "plt.plot(range(1, output_length + 1), y_pred_galformer[0], label=\"Predicted Prices (Galformer)\", marker='x')\n",
    "\n",
    "plt.title(\"Actual vs Predicted Prices (First Test Sample - Galformer)\")\n",
    "plt.xlabel(\"Days Ahead\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a6603",
   "metadata": {},
   "source": [
    "## Save the Galformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c650a0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "galformer_model.save('generalized_stock_galformer_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acfedee",
   "metadata": {},
   "source": [
    "## Inference with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e840aba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load necessary data for inference\n",
    "def load_company_data():\n",
    "    all_dfs = {}\n",
    "    parquet_files = glob.glob('df_*.parquet')\n",
    "    for file in parquet_files:\n",
    "        key = file.split('.')[0]  # e.g., 'df_AAPL'\n",
    "        df = pd.read_parquet(file)\n",
    "        all_dfs[key] = df\n",
    "    return all_dfs\n",
    "\n",
    "all_dfs = load_company_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bc9f8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_inference_data(company_df, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Prepare input data for inference for a single company.\n",
    "    Args:\n",
    "        company_df (DataFrame): The DataFrame for a specific company.\n",
    "        sequence_length (int): The number of past days to consider as input.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: The input data ready for prediction.\n",
    "    \"\"\"\n",
    "    # Ensure data is sorted by date\n",
    "    company_df = company_df.sort_index()\n",
    "\n",
    "    # Select relevant input features (exclude targets)\n",
    "    input_features = company_df.filter(regex=\"^(?!.*target).*\").values\n",
    "\n",
    "    # Take the last `sequence_length` days as input for prediction\n",
    "    if len(input_features) >= sequence_length:\n",
    "        input_sequence = input_features[-sequence_length:]\n",
    "        # Ensure the input sequence has the correct shape\n",
    "        input_sequence = np.expand_dims(input_sequence, axis=0)  # Add batch dimension\n",
    "        return input_sequence.astype(np.float32)  # Ensure data type consistency\n",
    "    else:\n",
    "        raise ValueError(\"Insufficient data for inference (less than sequence length).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2817f9f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_galformer_predictions_for_company(company_df, galformer_model, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Get Galformer predictions for a single company using the trained model.\n",
    "    Args:\n",
    "        company_df (DataFrame): DataFrame of the company.\n",
    "        galformer_model: Trained Galformer model.\n",
    "        sequence_length (int): Number of past days to consider as input.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Predicted prices.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare data for inference\n",
    "        input_data = prepare_inference_data(company_df, sequence_length=sequence_length)\n",
    "        \n",
    "        # Add positional encoding to inference data\n",
    "        pe = positional_encoding(input_data.shape[1], input_data.shape[2])\n",
    "        pe = tf.expand_dims(pe, axis=0)  # Shape: (1, sequence_length, num_features)\n",
    "        input_data_with_pe = input_data + pe.numpy()\n",
    "        \n",
    "        # Make predictions with Galformer\n",
    "        pred_galformer = galformer_model.predict(input_data_with_pe)\n",
    "        return pred_galformer.flatten()\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3ecfd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the trained Galformer model\n",
    "galformer_model = load_model('generalized_stock_galformer_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247835c8",
   "metadata": {},
   "source": [
    "### Making Predictions for All Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd37cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to get predictions for all companies\n",
    "def get_galformer_predictions_for_all_companies(all_dfs, galformer_model, sequence_length=60):\n",
    "    all_predictions = {}\n",
    "    for company_key, company_df in all_dfs.items():\n",
    "        print(f\"Processing {company_key}...\")\n",
    "        predictions = get_galformer_predictions_for_company(company_df, galformer_model, sequence_length=sequence_length)\n",
    "        if predictions is not None:\n",
    "            all_predictions[company_key] = predictions\n",
    "        else:\n",
    "            print(f\"Predictions not available for {company_key}.\")\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad48a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Get predictions for all companies\n",
    "all_company_predictions = get_galformer_predictions_for_all_companies(all_dfs, galformer_model, sequence_length=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d2cf6",
   "metadata": {},
   "source": [
    "### Saving Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to DataFrame for further analysis or saving\n",
    "def predictions_to_dataframe(predictions_dict):\n",
    "    records = []\n",
    "    for company_key, pred_values in predictions_dict.items():\n",
    "        for day_ahead, value in enumerate(pred_values, start=1):\n",
    "            records.append({\n",
    "                'Company': company_key.replace('df_', ''),\n",
    "                'Day_Ahead': day_ahead,\n",
    "                'Predicted_Price': value\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "predictions_df = predictions_to_dataframe(all_company_predictions)\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions DataFrame to a CSV file\n",
    "predictions_df.to_csv('galformer_stock_price_predictions.csv', index=False)\n",
    "print(\"Galformer Predictions have been saved to 'galformer_stock_price_predictions.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
